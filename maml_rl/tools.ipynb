{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from gym.spaces.box import Box\n",
    "from bc_gym_planning_env.envs.base import spaces\n",
    "from bc_gym_planning_env.envs.base.action import Action\n",
    "from bc_gym_planning_env.envs.egocentric import EgocentricCostmap\n",
    "from bc_gym_planning_env.envs.base.params import EnvParams\n",
    "from bc_gym_planning_env.robot_models.standard_robot_names_examples import StandardRobotExamples\n",
    "\n",
    "from bc_gym_planning_env.envs.mini_env import RandomMiniEnv\n",
    "from bc_gym_planning_env.envs.synth_turn_env import RandomAisleTurnEnv\n",
    "\n",
    "from gym_wrapper import bc_gym_wrapper\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from torch.nn.utils.convert_parameters import (vector_to_parameters,\n",
    "                                               parameters_to_vector)\n",
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchEpisodes(object):\n",
    "    def __init__(self, batch_size, device, gamma=0.95):\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        self._observations_list = [[] for _ in range(batch_size)]\n",
    "        self._actions_list = [[] for _ in range(batch_size)]\n",
    "        self._rewards_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "#         self._observations_list = None\n",
    "#         self._actions_list = None\n",
    "#         self._rewards_list = None\n",
    "#         self._mask_list = []\n",
    "\n",
    "        self._observations = None\n",
    "        self._actions = None\n",
    "        self._rewards = None\n",
    "        self._returns = None\n",
    "        self._mask = None\n",
    "\n",
    "    @property\n",
    "    def observations(self):\n",
    "        if self._observations is None:\n",
    "            observation_shape = self._observations_list[0][0].shape\n",
    "            observations = np.zeros((len(self), self.batch_size)\n",
    "                + observation_shape, dtype=np.float32)\n",
    "            for i in range(self.batch_size):\n",
    "                length = len(self._observations_list[i])\n",
    "                observations[:length, i] = np.stack(self._observations_list[i], axis=0)\n",
    "            self._observations = torch.from_numpy(observations).to(self.device)\n",
    "        return self._observations\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        if self._actions is None:\n",
    "            action_shape = self._actions_list[0][0].shape\n",
    "            actions = np.zeros((len(self), self.batch_size)\n",
    "                + action_shape, dtype=np.float32)\n",
    "            for i in range(self.batch_size):\n",
    "                length = len(self._actions_list[i])\n",
    "                actions[:length, i] = np.stack(self._actions_list[i], axis=0)\n",
    "            self._actions = torch.from_numpy(actions).to(self.device)\n",
    "        return self._actions\n",
    "\n",
    "    @property\n",
    "    def rewards(self):\n",
    "        if self._rewards is None:\n",
    "            rewards = np.zeros((len(self), self.batch_size), dtype=np.float32)\n",
    "            for i in range(self.batch_size):\n",
    "                length = len(self._rewards_list[i])\n",
    "                rewards[:length, i] = np.stack(self._rewards_list[i], axis=0)\n",
    "            self._rewards = torch.from_numpy(rewards).to(self.device)\n",
    "        return self._rewards\n",
    "\n",
    "    @property\n",
    "    def returns(self):\n",
    "        if self._returns is None:\n",
    "            return_ = np.zeros(self.batch_size, dtype=np.float32)\n",
    "            returns = np.zeros((len(self), self.batch_size), dtype=np.float32)\n",
    "            rewards = self.rewards.cpu().numpy()\n",
    "            mask = self.mask.cpu().numpy()\n",
    "            for i in range(len(self) - 1, -1, -1):\n",
    "                return_ = self.gamma * return_ + rewards[i] * mask[i]\n",
    "                returns[i] = return_\n",
    "            self._returns = torch.from_numpy(returns).to(self.device)\n",
    "        return self._returns\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        if self._mask is None:\n",
    "            mask = np.zeros((len(self), self.batch_size), dtype=np.float32)\n",
    "            for i in range(self.batch_size):\n",
    "                length = len(self._actions_list[i])\n",
    "                mask[:length, i] = 1.0\n",
    "            self._mask = torch.from_numpy(mask).to(self.device)\n",
    "        return self._mask\n",
    "\n",
    "    def gae(self, values, tau=1.0):\n",
    "        # Add an additional 0 at the end of values for\n",
    "        # the estimation at the end of the episode\n",
    "        values = values.squeeze(2).detach()\n",
    "        values = F.pad(values * self.mask, (0, 0, 0, 1))\n",
    "\n",
    "        deltas = self.rewards + self.gamma * values[1:] - values[:-1]\n",
    "        advantages = torch.zeros_like(deltas).float()\n",
    "        gae = torch.zeros_like(deltas[0]).float()\n",
    "        for i in range(len(self) - 1, -1, -1):\n",
    "            gae = gae * self.gamma * tau + deltas[i]\n",
    "            advantages[i] = gae\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def append(self, observation, action, reward, batch_id):\n",
    "            self._observations_list[batch_id].append(observation.astype(np.float32))\n",
    "            self._actions_list[batch_id].append(action.astype(np.float32))\n",
    "            self._rewards_list[batch_id].append(np.float32(reward))\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(map(len, self._rewards_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(object):\n",
    "    def __init__(self, env_name, meta_iter, batch_size, device):\n",
    "        if env_name == 'RandomMiniEnv':\n",
    "            self.env = RandomMiniEnv\n",
    "        elif env_name == 'RandomAisleTurnEnv':\n",
    "            self.env = RandomAisleTurnEnv\n",
    "    \n",
    "        self.meta_iter = meta_iter\n",
    "        self.batch_size = batch_size        \n",
    "        self.device = device\n",
    "\n",
    "    def sample_tasks(self, low, high, num_tasks):\n",
    "        seeds = np.random.randint(low=low, high=high, size=num_tasks)\n",
    "        tasks = []\n",
    "\n",
    "        env_param = EnvParams(iteration_timeout=200,\n",
    "                              goal_ang_dist=np.pi/8,\n",
    "                              goal_spat_dist=0.2,\n",
    "                              robot_name=StandardRobotExamples.INDUSTRIAL_TRICYCLE_V1)\n",
    "\n",
    "        for s in seeds:\n",
    "            env = EgocentricCostmap(self.env(params=env_param, \n",
    "                                             turn_off_obstacles=False,\n",
    "                                             draw_new_turn_on_reset=False,\n",
    "                                             seed=s))\n",
    "            env = bc_gym_wrapper(env)\n",
    "            \n",
    "            tasks.append(env)\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "    \n",
    "    def generate_episodes(self, task, policy, num_episodes):        \n",
    "        episodes = BatchEpisodes(batch_size=self.batch_size, device=self.device)\n",
    "        \n",
    "        traj_id = 0\n",
    "        \n",
    "        done = False\n",
    "        state = task.reset()\n",
    "        \n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action = policy(torch.Tensor(state).to(device=self.device)).sample()\n",
    "                action = action.cpu().numpy()\n",
    "                \n",
    "            next_state, reward, done, _ = task.step(action)\n",
    "            episodes.append(next_state, action, reward, traj_id)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                traj_id += 1\n",
    "\n",
    "                if traj_id == num_episodes:\n",
    "                    return episodes\n",
    "\n",
    "                done = False\n",
    "                state = task.reset()\n",
    "\n",
    "#     def one_traj(self, observations, actions, rewards, task, policy, traj_id, device):        \n",
    "#         done = False\n",
    "#         state = task.reset()\n",
    "\n",
    "#         while not done:\n",
    "#             with torch.no_grad():\n",
    "#                 action = policy(torch.Tensor(state).to(device)).sample()\n",
    "#                 action = action.cpu().numpy()\n",
    "\n",
    "#             next_state, reward, done, _ = task.step(action)\n",
    "\n",
    "#             observations[traj_id].append(next_state.astype(np.float32))\n",
    "#             actions[traj_id].append(action.astype(np.float32))\n",
    "#             rewards[traj_id].append(np.float32(reward))\n",
    "\n",
    "#             state = next_state\n",
    "            \n",
    "#     def generate_episodes(self, task, policy, num_episodes, device='cpu'):\n",
    "#         episodes = BatchEpisodes(batch_size=self.batch_size, device=device)\n",
    "        \n",
    "#         manager = mp.Manager()\n",
    "#         pool = mp.Pool(int(mp.cpu_count()/2))\n",
    "\n",
    "#         observations = manager.list()\n",
    "#         actions = manager.list()\n",
    "#         rewards = manager.list()\n",
    "        \n",
    "#         for i in range(self.batch_size):\n",
    "#             observations.append(manager.list())\n",
    "#             actions.append(manager.list())\n",
    "#             rewards.append(manager.list())\n",
    "        \n",
    "        \n",
    "#         import time; \n",
    "#         tic = time.process_time()\n",
    "        \n",
    "#         for i in range(self.batch_size):\n",
    "#             pool.apply_async(self.one_traj, args=(observations, actions, rewards, \n",
    "#                                                               task, policy, i, device))\n",
    "#         pool.close()\n",
    "#         pool.join()\n",
    "        \n",
    "#         toc = time.process_time()\n",
    "            \n",
    "#         print(\"time taken: %f\" % (toc-tic))\n",
    "            \n",
    "#         episodes._observations_list = list(map(lambda x : list(x), observations))\n",
    "#         episodes._actions_list = list(map(lambda x : list(x), actions))\n",
    "#         episodes._rewards_list = list(map(lambda x : list(x), rewards))\n",
    "                \n",
    "#         return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.zero_()\n",
    "            \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, init_std=1.0, min_std=1e-6):\n",
    "        super(Policy, self).__init__()\n",
    "        self.min_log_std = math.log(min_std)\n",
    "                \n",
    "        self.fc1 = nn.Linear(135, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "        \n",
    "        self.sigma = nn.Parameter(torch.Tensor(2))\n",
    "        self.sigma.data.fill_(math.log(init_std))\n",
    "        \n",
    "        self.apply(weight_init)\n",
    "        \n",
    "    def forward(self, input, params=None):\n",
    "        if params is None:\n",
    "            params = OrderedDict(self.named_parameters())\n",
    "                    \n",
    "        output = F.relu(F.linear(input, weight=params['fc1.weight'], bias=params['fc1.bias']))\n",
    "        output = F.relu(F.linear(output, weight=params['fc2.weight'], bias=params['fc2.bias']))\n",
    "        output = F.linear(output, weight=params['fc3.weight'], bias=params['fc3.bias'])\n",
    "        \n",
    "        scale = torch.exp(torch.clamp(params['sigma'], min=self.min_log_std))\n",
    "        \n",
    "        return Normal(loc=output, scale=scale)\n",
    "    \n",
    "    \n",
    "    def update_params(self, loss, step_size=0.5, first_order=False):\n",
    "        grads = torch.autograd.grad(loss, self.parameters(),\n",
    "            create_graph=not first_order)\n",
    "        \n",
    "        updated_params = OrderedDict()\n",
    "        \n",
    "        for (name, param), grad in zip(self.named_parameters(), grads):\n",
    "            updated_params[name] = param - step_size * grad\n",
    "\n",
    "        return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFeatureBaseline(nn.Module):\n",
    "    def __init__(self, input_size, reg_coeff=1e-5):\n",
    "        super(LinearFeatureBaseline, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self._reg_coeff = reg_coeff\n",
    "        self.linear = nn.Linear(self.feature_size, 1, bias=False)\n",
    "        self.linear.weight.data.zero_()\n",
    "\n",
    "    @property\n",
    "    def feature_size(self):\n",
    "        return 2 * self.input_size + 4\n",
    "\n",
    "    def _feature(self, episodes):\n",
    "        ones = episodes.mask.unsqueeze(2)\n",
    "        observations = episodes.observations * ones\n",
    "        cum_sum = torch.cumsum(ones, dim=0) * ones\n",
    "        al = cum_sum / 100.0\n",
    "\n",
    "        return torch.cat([observations, observations ** 2,\n",
    "            al, al ** 2, al ** 3, ones], dim=2)\n",
    "\n",
    "    def fit(self, episodes):\n",
    "        # sequence_length * batch_size x feature_size\n",
    "        featmat = self._feature(episodes).view(-1, self.feature_size)\n",
    "        # sequence_length * batch_size x 1\n",
    "        returns = episodes.returns.view(-1, 1)\n",
    "\n",
    "        reg_coeff = self._reg_coeff\n",
    "        eye = torch.eye(self.feature_size, dtype=torch.float32,\n",
    "            device=self.linear.weight.device)\n",
    "\n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                arr1 = torch.matmul(featmat.t(), returns).cpu().numpy()\n",
    "                arr2 = (torch.matmul(featmat.t(), featmat) + reg_coeff * eye).cpu().numpy()\n",
    "                \n",
    "                coeffs, _, _, _ = np.linalg.lstsq(arr1, arr2, rcond=1)\n",
    "                coeffs = torch.Tensor(coeffs).to(self.linear.weight.device)\n",
    "                coeffs = torch.transpose(coeffs, 0, 1)\n",
    "                \n",
    "#                 coeffs1, _ = torch.gels(\n",
    "#                     torch.matmul(featmat.t(), returns),\n",
    "#                     torch.matmul(featmat.t(), featmat) + reg_coeff * eye\n",
    "#                 )\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                reg_coeff += 10\n",
    "        else:\n",
    "            raise RuntimeError('Unable to solve the normal equations in '\n",
    "                '`LinearFeatureBaseline`. The matrix X^T*X (with X the design '\n",
    "                'matrix) is not full-rank, regardless of the regularization '\n",
    "                '(maximum regularization: {0}).'.format(reg_coeff))\n",
    "        self.linear.weight.data = coeffs.data.t()\n",
    "\n",
    "    def forward(self, episodes):\n",
    "        features = self._feature(episodes)\n",
    "        return self.linear(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
    "    p = b.clone().detach()\n",
    "    r = b.clone().detach()\n",
    "    x = torch.zeros_like(b).float()\n",
    "    rdotr = torch.dot(r, r)\n",
    "\n",
    "    for i in range(cg_iters):\n",
    "        z = f_Ax(p).detach()\n",
    "        v = rdotr / torch.dot(p, z)\n",
    "        x += v * p\n",
    "        r -= v * z\n",
    "        newrdotr = torch.dot(r, r)\n",
    "        mu = newrdotr / rdotr\n",
    "        p = r + mu * p\n",
    "\n",
    "        rdotr = newrdotr\n",
    "        if rdotr.item() < residual_tol:\n",
    "            break\n",
    "\n",
    "    return x.detach()\n",
    "\n",
    "def weighted_mean(tensor, dim=None, weights=None):\n",
    "    if weights is None:\n",
    "        out = torch.mean(tensor)\n",
    "    if dim is None:\n",
    "        out = torch.sum(tensor * weights)\n",
    "        out.div_(torch.sum(weights))\n",
    "    else:\n",
    "        mean_dim = torch.sum(tensor * weights, dim=dim)\n",
    "        mean_dim.div_(torch.sum(weights, dim=dim))\n",
    "        out = torch.mean(mean_dim)\n",
    "        \n",
    "    return out\n",
    "\n",
    "def weighted_normalize(tensor, dim=None, weights=None, epsilon=1e-8):\n",
    "    mean = weighted_mean(tensor, dim=dim, weights=weights)\n",
    "    out = tensor * (1 if weights is None else weights) - mean\n",
    "    std = torch.sqrt(weighted_mean(out ** 2, dim=dim, weights=weights))\n",
    "    out.div_(std + epsilon)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def detach_distribution(pi):\n",
    "    distribution = Normal(loc=pi.loc.detach(), scale=pi.scale.detach())\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner(object):\n",
    "    def __init__(self, sampler, policy, baseline, gamma=0.95,\n",
    "                 fast_lr=0.01, tau=1.0, device='cpu'):\n",
    "        self.sampler = sampler\n",
    "        self.policy = policy\n",
    "        self.baseline = baseline\n",
    "        self.gamma = gamma\n",
    "        self.fast_lr = fast_lr\n",
    "        self.tau = tau\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def inner_loss(self, episodes, params=None):\n",
    "        \"\"\"Compute the inner loss for the one-step gradient update. The inner \n",
    "        loss is REINFORCE with baseline [2], computed on advantages estimated \n",
    "        with Generalized Advantage Estimation (GAE, [3]).\n",
    "        \"\"\"\n",
    "        values = self.baseline(episodes)\n",
    "        advantages = episodes.gae(values, tau=self.tau)\n",
    "        advantages = weighted_normalize(advantages, weights=episodes.mask)\n",
    "\n",
    "        pi = self.policy(episodes.observations, params=params)\n",
    "        log_probs = pi.log_prob(episodes.actions)\n",
    "        if log_probs.dim() > 2:\n",
    "            log_probs = torch.sum(log_probs, dim=2)\n",
    "        loss = -weighted_mean(log_probs * advantages, dim=0,\n",
    "            weights=episodes.mask)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def adapt(self, episodes, first_order=False):\n",
    "        \"\"\"Adapt the parameters of the policy network to a new task, from \n",
    "        sampled trajectories `episodes`, with a one-step gradient update [1].\n",
    "        \"\"\"\n",
    "        # Fit the baseline to the training episodes\n",
    "        self.baseline.fit(episodes)\n",
    "        # Get the loss on the training episodes\n",
    "        loss = self.inner_loss(episodes)\n",
    "        # Get the new parameters after a one-step gradient update\n",
    "        params = self.policy.update_params(loss, step_size=self.fast_lr,\n",
    "            first_order=first_order)\n",
    "\n",
    "        return params\n",
    "\n",
    "    def sample(self, tasks, first_order=False):\n",
    "        \"\"\"Sample trajectories (before and after the update of the parameters) \n",
    "        for all the tasks `tasks`.\n",
    "        \"\"\"\n",
    "        \n",
    "        episodes = []\n",
    "        for i, task in enumerate(tasks):            \n",
    "#             print(\"Starting sampling for task %d\" % i)\n",
    "                                                                \n",
    "            train_episodes = self.sampler.generate_episodes(task, self.policy, num_episodes=20)\n",
    "                        \n",
    "#             print(\"Train episodes for task %d complete\" % i)\n",
    "\n",
    "            params = self.adapt(train_episodes, first_order=first_order)\n",
    "            \n",
    "#             print(\"Done fitting baseline for task %d\" % i)\n",
    "\n",
    "            valid_episodes = self.sampler.generate_episodes(task, self.policy, num_episodes=20)\n",
    "            \n",
    "#             print(\"Validation episodes for task %d complete\" % i)\n",
    "            \n",
    "            episodes.append((train_episodes, valid_episodes))\n",
    "            \n",
    "#             print(\"Finished sampling for task %d\" % i)\n",
    "#             print()\n",
    "                        \n",
    "        return episodes\n",
    "    \n",
    "    def average_return(self, episodes):\n",
    "        total_return = 0\n",
    "        \n",
    "        for _, valid_episodes in episodes:\n",
    "            total_return += torch.sum(valid_episodes.rewards)\n",
    "                    \n",
    "        return total_return.item()/(len(episodes) * self.sampler.batch_size)\n",
    "    \n",
    "    def test_accuracy(self, sampler):\n",
    "        task = sampler.sample_tasks(low=sampler.meta_iter * sampler.batch_size + 1,\n",
    "                                    high=sampler.meta_iter * sampler.batch_size + 100,\n",
    "                                    num_tasks=1)[0]\n",
    "        \n",
    "        test_episodes = sampler.generate_episodes(task, self.policy, num_episodes=40)\n",
    "        \n",
    "        return torch.sum(test_episodes.rewards >= 200).item()/40\n",
    "\n",
    "\n",
    "    def kl_divergence(self, episodes, old_pis=None):\n",
    "        kls = []\n",
    "        if old_pis is None:\n",
    "            old_pis = [None] * len(episodes)\n",
    "\n",
    "        for (train_episodes, valid_episodes), old_pi in zip(episodes, old_pis):\n",
    "            params = self.adapt(train_episodes)\n",
    "            pi = self.policy(valid_episodes.observations, params=params)\n",
    "\n",
    "            if old_pi is None:\n",
    "                old_pi = detach_distribution(pi)\n",
    "\n",
    "            mask = valid_episodes.mask\n",
    "            if valid_episodes.actions.dim() > 2:\n",
    "                mask = mask.unsqueeze(2)\n",
    "            kl = weighted_mean(kl_divergence(pi, old_pi), dim=0, weights=mask)\n",
    "            kls.append(kl)\n",
    "\n",
    "        return torch.mean(torch.stack(kls, dim=0))\n",
    "\n",
    "    def hessian_vector_product(self, episodes, damping=1e-2):\n",
    "        \"\"\"Hessian-vector product, based on the Perlmutter method.\"\"\"\n",
    "        def _product(vector):\n",
    "            kl = self.kl_divergence(episodes)\n",
    "            grads = torch.autograd.grad(kl, self.policy.parameters(),\n",
    "                create_graph=True)\n",
    "            flat_grad_kl = parameters_to_vector(grads)\n",
    "\n",
    "            grad_kl_v = torch.dot(flat_grad_kl, vector)\n",
    "            grad2s = torch.autograd.grad(grad_kl_v, self.policy.parameters())\n",
    "            flat_grad2_kl = parameters_to_vector(grad2s)\n",
    "\n",
    "            return flat_grad2_kl + damping * vector\n",
    "        return _product\n",
    "\n",
    "    def surrogate_loss(self, episodes, old_pis=None):\n",
    "        losses, kls, pis = [], [], []\n",
    "        if old_pis is None:\n",
    "            old_pis = [None] * len(episodes)\n",
    "\n",
    "        for (train_episodes, valid_episodes), old_pi in zip(episodes, old_pis):\n",
    "            params = self.adapt(train_episodes)\n",
    "            with torch.set_grad_enabled(old_pi is None):\n",
    "                pi = self.policy(valid_episodes.observations, params=params)\n",
    "                pis.append(detach_distribution(pi))\n",
    "\n",
    "                if old_pi is None:\n",
    "                    old_pi = detach_distribution(pi)\n",
    "\n",
    "                values = self.baseline(valid_episodes)\n",
    "                advantages = valid_episodes.gae(values, tau=self.tau)\n",
    "                advantages = weighted_normalize(advantages,\n",
    "                    weights=valid_episodes.mask)\n",
    "\n",
    "                log_ratio = (pi.log_prob(valid_episodes.actions)\n",
    "                    - old_pi.log_prob(valid_episodes.actions))\n",
    "                if log_ratio.dim() > 2:\n",
    "                    log_ratio = torch.sum(log_ratio, dim=2)\n",
    "                ratio = torch.exp(log_ratio)\n",
    "\n",
    "                loss = -weighted_mean(ratio * advantages, dim=0,\n",
    "                    weights=valid_episodes.mask)\n",
    "                losses.append(loss)\n",
    "\n",
    "                mask = valid_episodes.mask\n",
    "                if valid_episodes.actions.dim() > 2:\n",
    "                    mask = mask.unsqueeze(2)\n",
    "                kl = weighted_mean(kl_divergence(pi, old_pi), dim=0,\n",
    "                    weights=mask)\n",
    "                kls.append(kl)\n",
    "\n",
    "        return (torch.mean(torch.stack(losses, dim=0)),\n",
    "                torch.mean(torch.stack(kls, dim=0)), pis)\n",
    "\n",
    "    def step(self, episodes, max_kl=1e-3, cg_iters=10, cg_damping=1e-2,\n",
    "             ls_max_steps=10, ls_backtrack_ratio=0.5):\n",
    "        \"\"\"Meta-optimization step (ie. update of the initial parameters), based \n",
    "        on Trust Region Policy Optimization (TRPO, [4]).\n",
    "        \"\"\"\n",
    "        old_loss, _, old_pis = self.surrogate_loss(episodes)\n",
    "        \n",
    "        loss = old_loss.item()\n",
    "        \n",
    "        grads = torch.autograd.grad(old_loss, self.policy.parameters())\n",
    "        grads = parameters_to_vector(grads)\n",
    "\n",
    "        # Compute the step direction with Conjugate Gradient\n",
    "        hessian_vector_product = self.hessian_vector_product(episodes,\n",
    "            damping=cg_damping)\n",
    "        stepdir = conjugate_gradient(hessian_vector_product, grads,\n",
    "            cg_iters=cg_iters)\n",
    "\n",
    "        # Compute the Lagrange multiplier\n",
    "        shs = 0.5 * torch.dot(stepdir, hessian_vector_product(stepdir))\n",
    "        lagrange_multiplier = torch.sqrt(shs / max_kl)\n",
    "\n",
    "        step = stepdir / lagrange_multiplier\n",
    "\n",
    "        # Save the old parameters\n",
    "        old_params = parameters_to_vector(self.policy.parameters())\n",
    "\n",
    "        # Line search\n",
    "        step_size = 1.0\n",
    "        for _ in range(ls_max_steps):\n",
    "            vector_to_parameters(old_params - step_size * step,\n",
    "                                 self.policy.parameters())\n",
    "            loss, kl, _ = self.surrogate_loss(episodes, old_pis=old_pis)\n",
    "            improve = loss - old_loss\n",
    "            if (improve.item() < 0.0) and (kl.item() < max_kl):\n",
    "                break\n",
    "            step_size *= ls_backtrack_ratio\n",
    "        else:\n",
    "            vector_to_parameters(old_params, self.policy.parameters())\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def to(self, device, **kwargs):\n",
    "        self.policy.to(device, **kwargs)\n",
    "        self.baseline.to(device, **kwargs)\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
